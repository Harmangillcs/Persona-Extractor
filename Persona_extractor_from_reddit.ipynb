from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.prompts import PromptTemplate
from langchain.llms import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
from langchain.chains import LLMChain
import os
import json
from datetime import datetime
from getpass import getpass
import praw

#Upload your reddit id and secret key:

client_id = getpass("Enter your Reddit client_id: ")
client_secret = getpass("Enter your Reddit client_secret: ")
user_agent = "persona-extractor-script by u/SwimmingIndividual61"

reddit = praw.Reddit(
    client_id=client_id,
    client_secret=client_secret,
    user_agent=user_agent
)

#extract comments and posts from reddit page link: 
user = reddit.redditor("kojied")

data = []

for comment in user.comments.new(limit=100):
    data.append(comment.body)

for post in user.submissions.new(limit=100):
    text = post.title + "\n" + post.selftext
    data.append(text)

#combine all data 
combined_text = "\n\n".join(data)
combined_text

#Create chunks 
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,
    chunk_overlap=200
)

chunks = text_splitter.create_documents([combined_text])
chunks

#Download embedding model
embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

#Create vectorstore 
vectordb=FAISS.from_documents(chunks,embedding=embeddings)

#Login hugging face account
!huggingface-cli login

#Download model
model_id = "HuggingFaceTB/SmolLM3-3B"  #or you can use gpt-3.5-turbo model for better answers than this
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_4bit=True
    )

pipe=pipeline("text-generation", model=model, tokenizer=tokenizer,max_new_tokens=300, do_sample=True, temperature=0.3)
llm=HuggingFacePipeline(pipeline=pipe)

#Create Template
prompt = PromptTemplate(
    input_variables=["text"],
    template="""
You are a UX researcher creating a concise user persona based on Reddit posts by a single user.

Only analyze one user donot try to create persona of multiple users.. Do not repeat Reddit content.

Format:

Username: [Guess if not given]

Personality:
- [Each trait on its own line.]

Hobbies & Interests:
- [List separately with line breaks.]

Life Stage:
- [Short sentence.]

Goals & Motivations:
- [One point per line.]

Frustrations:
- [One point per line.]

Writing Style:
- [One line per observation.]

Demographic Clues:
- [Each clue on its own line.]

Citation:
- ["Short quote from the Reddit text."]

Only return the persona, clearly formatted. If the user is unclear, infer a realistic alias.
"""
)


#initialize llm chain
qa = LLMChain(llm=llm, prompt=prompt)

#Combine all chunks are generate result
combined_text = "\n\n".join(chunk.page_content.strip() for chunk in chunks)
response = qa.invoke({
    "text": combined_text
})
print(response["text"].strip())

#Save the output in text file 
with open("persona_output.txt", "w", encoding="utf-8") as f:
    f.write(response["text"])

print("persona_output.txt")



